{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "rap_text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09eeeR5prIJ"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJFiukHTO0IK"
      },
      "source": [
        "This Python Notebook is created by TensorFlow, I have modified the input file and parameters \n",
        "\n",
        "The original file can be found here https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_55cOxLkAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fed88e0-dc4d-40f1-e190-cfd0b3601b38"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('Rap.txt', 'https://raw.githubusercontent.com/CC-94/txtfiles/main/Rap.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/CC-94/txtfiles/main/Rap.txt\n",
            "417792/411290 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34cbe349-c7fa-44d8-ff5b-e965c6a81775"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 410229 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb640d1-3363-4535-cd7d-d99014eb937b"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Intro] Ricky Racks, I see you! Thugger! YSL for life, bitch Yeah fuck you, your momma and everything else Yung Shad, you killed this track Free the GOAT! [Chorus] That's my best friend, that's my best friend, flexin' Big ol' booty bitch missus from \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0644890-ada1-414e-fc77-61042df3f870"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a86OoYtO01go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ad5b4e-5739-4a3f-9ef9-90e3283b1007"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21079f7-75f4-4db5-fd7d-a7de981a23ec"
      },
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[58, 59, 60, 61, 62, 63, 64], [81, 82, 83]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2GCh0ySD44s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32645115-6cea-4ee1-cb64-977afb358c91"
      },
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e19f038-b230-4b96-f5ee-e34a0a55fb7c"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UopbsKi88tm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339196ef-0559-4705-adb3-a38cd015e49d"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(410229,), dtype=int64, numpy=array([56, 38, 71, ..., 65, 62, 75])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjH5v45-yqqH"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1a821b-d340-4631-b1ad-66e8fccc5266"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'[' b'I' b'n' b't' b'r' b'o' b']' b' ' b'R' b'i' b'c' b'k' b'y' b' '\n",
            " b'R' b'a' b'c' b'k' b's' b',' b' ' b'I' b' ' b's' b'e' b'e' b' ' b'y'\n",
            " b'o' b'u' b'!' b' ' b'T' b'h' b'u' b'g' b'g' b'e' b'r' b'!' b' ' b'Y'\n",
            " b'S' b'L' b' ' b'f' b'o' b'r' b' ' b'l' b'i' b'f' b'e' b',' b' ' b'b'\n",
            " b'i' b't' b'c' b'h' b' ' b'Y' b'e' b'a' b'h' b' ' b'f' b'u' b'c' b'k'\n",
            " b' ' b'y' b'o' b'u' b',' b' ' b'y' b'o' b'u' b'r' b' ' b'm' b'o' b'm'\n",
            " b'm' b'a' b' ' b'a' b'n' b'd' b' ' b'e' b'v' b'e' b'r' b'y' b't' b'h'\n",
            " b'i' b'n' b'g'], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO32cMWu4a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea4cf7b-ed29-441b-ba88-568a51660875"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'[Intro] Ricky Racks, I see you! Thugger! YSL for life, bitch Yeah fuck you, your momma and everything'\n",
            "b\" else Yung Shad, you killed this track Free the GOAT! [Chorus] That's my best friend, that's my best \"\n",
            "b\"friend, flexin' Big ol' booty bitch missus from Texas, what's next is I'm gon' skeet off, lil' nigga \"\n",
            "b\"come catch me, catch me And that's my bestie, my bestie, my best friend, go best friend Nigga livin' \"\n",
            "b\"TTG and everything is still on fleek Bad bitch rollin' wit' me, she gon' smile 'cause she on fleek Hu\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxbDTJTw5u_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3be0de8-70c8-42b5-97a0-3a9125bf3245"
      },
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95dd6a0a-d3ff-4f3f-c55f-bf7c2b207540"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'[Intro] Ricky Racks, I see you! Thugger! YSL for life, bitch Yeah fuck you, your momma and everythin'\n",
            "Target: b'Intro] Ricky Racks, I see you! Thugger! YSL for life, bitch Yeah fuck you, your momma and everything'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7871c3e-9042-4690-82e5-df8f56c431b7"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d952b7a4-aa6d-4e48-d870-9a79538f7833"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 102) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c60b1bfc-487b-4151-cea1-86217cc8a48c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  26112     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  104550    \n",
            "=================================================================\n",
            "Total params: 4,068,966\n",
            "Trainable params: 4,068,966\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqFMUQc_UFgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db96c52-9bd0-4ddb-b6dd-afcb29736137"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 28,  28,   5,  19,  53,  35,  10,  83,  63,  97,   4,  46,  20,\n",
              "        34,  55, 101,   7,   6,  15,  21,  14,  26,  75,  33,  88,  62,\n",
              "         2,  48,  57,  11,  89,  88,  82,  63,  89,  19,  88,  57,   4,\n",
              "        85,  52,  59,  74,  50,  63,   2,  81,   5,  36,  68,  43,  67,\n",
              "        10,  59,  77,  51,  69,  98,  88,  45,  76,  62,  76,  69,  29,\n",
              "        85,  77,  85,  74,  99,  60,  98,  28,  87,   2,  27,  18,  99,\n",
              "        41,  39,  44,  24,  37, 100,  55,   9,  19,  75,  69,  29,  12,\n",
              "        82,  39,   6,  64,   3,  52,  79,  32,  99])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWcFwPwLSo05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f2a30d-34b7-43e3-b8ff-1ea263a34564"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b\"on me) Watch what you say and watch what you do, 'cause I'll slime your OG (Slime, slime) In Bali, s\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b';;\"2XF(zf\\xe2\\x80\\x9d!Q3EZ\\xe2\\x84\\xa2%$.4-9rD\\xc2\\xa9e\\rS])\\xc2\\xb1\\xc2\\xa9yf\\xc2\\xb12\\xc2\\xa9]!\\xc2\\xa6WbqUf\\rx\"GkNj(btVl\\xe2\\x80\\xa6\\xc2\\xa9Psesl?\\xc2\\xa6t\\xc2\\xa6q\\xe2\\x80\\xb9c\\xe2\\x80\\xa6;\\xc2\\xa8\\r:1\\xe2\\x80\\xb9LJO7H\\xe2\\x82\\xacZ\\'2rl?*yJ$g WvC\\xe2\\x80\\xb9'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cceebc99-f382-4bdd-b13c-e80ff17c1c25"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 102)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.6258783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a6bf7a-f1da-4614-bf0e-2de2c2ebd502"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102.09241"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS = 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "341bc4b8-11e9-48d7-afb4-d4bffc3efa41"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "63/63 [==============================] - 4s 42ms/step - loss: 4.0283\n",
            "Epoch 2/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 2.5363\n",
            "Epoch 3/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 2.2446\n",
            "Epoch 4/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 2.0305\n",
            "Epoch 5/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 1.8503\n",
            "Epoch 6/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 1.7243\n",
            "Epoch 7/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 1.6272\n",
            "Epoch 8/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 1.5239\n",
            "Epoch 9/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 1.4390\n",
            "Epoch 10/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 1.3490\n",
            "Epoch 11/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 1.2733\n",
            "Epoch 12/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 1.2002\n",
            "Epoch 13/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 1.1115\n",
            "Epoch 14/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 1.0306\n",
            "Epoch 15/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.9553\n",
            "Epoch 16/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.8597\n",
            "Epoch 17/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.7783\n",
            "Epoch 18/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.7006\n",
            "Epoch 19/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.6224\n",
            "Epoch 20/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.5439\n",
            "Epoch 21/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.4720\n",
            "Epoch 22/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.4086\n",
            "Epoch 23/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.3436\n",
            "Epoch 24/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.3001\n",
            "Epoch 25/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.2568\n",
            "Epoch 26/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.2225\n",
            "Epoch 27/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1926\n",
            "Epoch 28/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1715\n",
            "Epoch 29/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1528\n",
            "Epoch 30/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1348\n",
            "Epoch 31/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1235\n",
            "Epoch 32/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1128\n",
            "Epoch 33/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1071\n",
            "Epoch 34/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1009\n",
            "Epoch 35/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0964\n",
            "Epoch 36/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0934\n",
            "Epoch 37/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0922\n",
            "Epoch 38/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0898\n",
            "Epoch 39/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0893\n",
            "Epoch 40/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0905\n",
            "Epoch 41/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0900\n",
            "Epoch 42/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0913\n",
            "Epoch 43/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0953\n",
            "Epoch 44/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1088\n",
            "Epoch 45/60\n",
            "63/63 [==============================] - 3s 43ms/step - loss: 0.1479\n",
            "Epoch 46/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.2342\n",
            "Epoch 47/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.2531\n",
            "Epoch 48/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1998\n",
            "Epoch 49/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1480\n",
            "Epoch 50/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.1116\n",
            "Epoch 51/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0906\n",
            "Epoch 52/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0803\n",
            "Epoch 53/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0744\n",
            "Epoch 54/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0716\n",
            "Epoch 55/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0705\n",
            "Epoch 56/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0701\n",
            "Epoch 57/60\n",
            "63/63 [==============================] - 3s 43ms/step - loss: 0.0691\n",
            "Epoch 58/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0678\n",
            "Epoch 59/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0680\n",
            "Epoch 60/60\n",
            "63/63 [==============================] - 3s 42ms/step - loss: 0.0676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=0.5):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae0a008-253e-49c5-e3bd-9bf7b0f2d62a"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Intro:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intro: Birdman & (Young Thug)] That Rich Gang lifestyle Marble floors, gold to the shots (I'm calling all the shots) I'm coolin', she say she bang gangs (Gah gat) Can't you tell? (Pi'erre) I'm in this bitch with Yung Gunna, yeah, ho, YSL (YSL, what?) Walk inside the club, I smell like YSL (What? What?) Fuckin' on your thot, your bitch, she wild as hell Ooh, I spend a check up on my watch, the frame is wide as hell (Wide as hell) Ooh, fuckin' on your thot, I smell like YSL (Beep, psh-beep, thot) In this bitch with Gunna, yeah, ho, YSL (YSL, yeah) [Verse 2: Gunna] Saint Laurent (Saint Laur, \"Damn) She the B-E-S-T, best (To) Prienies (Blowin' up the strip) I got the door unlocked And I know right now she'd rather have my dick than a watch (Facts) And she like it was don't comper than a wite (Ayy) Girl, I know she livin' for a nigga, dyin' for a nigga, baby? Would you die for a nigga? Hey Baby girl, would you cry for your nigga, baby? Ay, bitch, got 'em (Uh hundred chopser) When I drop that shit \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 1.7752885818481445\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}